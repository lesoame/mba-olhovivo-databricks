# ğŸšŒ SPTrans Olho Vivo | Databricks Lakehouse (Standard + GTFS)

![Status](https://img.shields.io/badge/Status-Active-success.svg)
![Databricks](https://img.shields.io/badge/Databricks-Standard-orange.svg)
![Azure](https://img.shields.io/badge/Cloud-Azure-blue.svg)
![Orchestration](https://img.shields.io/badge/Workflows-Native-green.svg)
![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.31+-red.svg)


> **Pipeline de Engenharia de Dados End-to-End** para monitoramento da frota de Ã´nibus de SÃ£o Paulo. Integra dados de telemetria em tempo real (API Olho Vivo) com dados estÃ¡ticos de planejamento (GTFS), processados em arquitetura Medallion (Bronze/Silver/Gold) no Databricks e visualizados em um Dashboard Streamlit com Chatbot.

---

## ğŸ—ï¸ Arquitetura e Fluxo de Dados

O projeto foi desenhado para operar com **eficiÃªncia de custos**, utilizando recursos do plano Standard do Databricks na Azure.

```mermaid
graph TD
    %% DefiniÃ§Ã£o de Estilos - CORES AJUSTADAS PARA TEXTO PRETO
    classDef config fill:#ffb3ba,stroke:#333,stroke-width:2px,color:#000;
    classDef bronze fill:#ffdfba,stroke:#333,stroke-width:2px,color:#000;
    classDef silver fill:#e0e0e0,stroke:#333,stroke-width:2px,color:#000;
    classDef gold fill:#ffffba,stroke:#333,stroke-width:2px,color:#000;
    classDef app fill:#bae1ff,stroke:#333,stroke-width:2px,color:#000;
    classDef source fill:#fff,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5,color:#000;

    subgraph Fontes_Externas ["Fontes de Dados"]
        API_OlhoVivo["API Olho Vivo SPTrans"]:::source
        GTFS_Files["Arquivos GTFS EstÃ¡ticos"]:::source
    end

    subgraph Env_Setup ["1. ConfiguraÃ§Ã£o do Ambiente"]
        Schemas["criacao_schemas.sql"]:::config
        Tab_Bronze["criacao_tabelas_bronze.sql"]:::config
        Tab_Silver["criacao_tabelas_silver.sql"]:::config
        Tab_Gold["criacao_tabelas_gold.sql"]:::config
        Tab_GTFS["criacao_tabelas_gtfs.py"]:::config
    end

    subgraph Bronze_Layer ["2. IngestÃ£o (Bronze)"]
        direction TB
        Ingest_Posicao["posicao_veiculos.py"]:::bronze
        Ingest_Linhas["buscar_linhas.py / buscar_linhas_sentido.py"]:::bronze
        Ingest_Paradas["buscar_paradas*.py"]:::bronze
        Ingest_Empresas["empresas.py / corredores.py"]:::bronze
        Ingest_Previsao["previsao_chegada*.py"]:::bronze
    end

    subgraph Silver_Layer ["3. Refinamento (Silver)"]
        direction TB
        Fato_Posicao["fato_posicao_veiculos.py"]:::silver
        Dim_Linhas["dim_linhas.py"]:::silver
        Dim_Empresas["dim_empresas.py"]:::silver
        Mapa_Shapes["mapa_shapes_gtfs.py"]:::silver
    end

    subgraph Gold_Layer ["4. AgregaÃ§Ã£o e KPIs (Gold)"]
        direction TB
        KPI_Velocidade["velocidade_linhas.py"]:::gold
        KPI_Snapshot["snapshot_frota_atual.py"]:::gold
        KPI_Acessibilidade["acessibilidade.py"]:::gold
        KPI_Rota["perfil_rota_estatico.sql"]:::gold
    end

    subgraph Visualization ["5. VisualizaÃ§Ã£o"]
        Streamlit_App["app/app.py"]:::app
    end

    %% ConexÃµes
    Schemas --> Tab_Bronze --> Tab_Silver --> Tab_Gold --> Tab_GTFS
    
    API_OlhoVivo --> Ingest_Posicao
    API_OlhoVivo --> Ingest_Linhas
    API_OlhoVivo --> Ingest_Paradas
    API_OlhoVivo --> Ingest_Empresas
    
    Tab_Bronze -.-> Ingest_Posicao
    
    Ingest_Posicao --> Fato_Posicao
    Ingest_Linhas --> Dim_Linhas
    Ingest_Empresas --> Dim_Empresas
    GTFS_Files --> Mapa_Shapes
    
    Fato_Posicao & Dim_Linhas --> KPI_Velocidade
    Fato_Posicao & Dim_Linhas & Dim_Empresas --> KPI_Snapshot
    Dim_Linhas & Fato_Posicao --> KPI_Acessibilidade
    Mapa_Shapes --> KPI_Rota
    
    KPI_Velocidade --> Streamlit_App
    KPI_Snapshot --> Streamlit_App
    KPI_Rota --> Streamlit_App
    KPI_Acessibilidade --> Streamlit_App
```


## âš™ï¸ OrquestraÃ§Ã£o (Databricks Workflows)

A automaÃ§Ã£o do pipeline Ã© gerenciada nativamente pelo **Databricks Workflows (Jobs)**, sem necessidade de ferramentas externas como Airflow.

| ParÃ¢metro | ConfiguraÃ§Ã£o |
| :--- | :--- |
| **Nome do Job** | `pipeline_olhovivo` |
| **FrequÃªncia** | A cada 15 minutos (Cron Schedule) |
| **Cluster** | Cluster All-Purpose (Standard Mode) |

### Tasks do Workflow (ExecuÃ§Ã£o Sequencial)

1. **`1_ing_posic_veic_bronze`**: Conecta na API e baixa o JSON raw.
2. **`2_posic_veic_silver`**: Processa, explode e limpa os dados.
3. **`3_velocidade_gold`**: Calcula a mÃ©dia de velocidade e tempo de viagem.
4. **`4_snapshot_mapa`**: Atualiza a Ãºltima posiÃ§Ã£o conhecida da frota.


## â˜ï¸ EstratÃ©gia de Infraestrutura e Custos (FinOps)

Este projeto adota uma arquitetura otimizada para reduzir custos de nuvem e licenciamento Databricks (DBUs), ideal para ambientes de desenvolvimento e POCs.

### 1. Armazenamento (Azure Storage vs. Catalog)
> - **Dados FÃ­sicos (Parquet/Delta):** Todos os dados persistem de forma segura em um **Azure Storage Account (ADLS Gen2)**.
> - **Metadados:** Utilizamos o **Hive Metastore (Legacy)** embutido no cluster, ao invÃ©s do Unity Catalog, para evitar custos adicionais de gerenciamento e complexidade de setup em workspace Standard.


### 2. Metadados EfÃªmeros (Cluster-Scoped)
Como estratÃ©gia de economia, utilizamos o metastore local do cluster (banco Derby embutido).
> - âš ï¸ **Comportamento:** Quando o cluster Ã© desligado/reiniciado, os ponteiros (schemas e definiÃ§Ãµes de tabelas) desaparecem da interface visual do Catalog.
> - ğŸ’¾ **PersistÃªncia:** Os dados **nÃ£o sÃ£o perdidos**, pois estÃ£o salvos fisicamente no Azure Storage.
> - ğŸ”„ **RecuperaÃ§Ã£o:** O pipeline inclui notebooks de "Ambiente" (`criacao_schemas`, `criacao_tabelas`) que recriam os ponteiros apontando para os locais existentes no Storage (`LOCATION 'abfss://...'`) sempre que o ambiente Ã© reiniciado.


## ğŸ§  LÃ³gica de NegÃ³cio (Camadas)

### ğŸ¥‰ Camada Bronze (IngestÃ£o Raw)
> **PosiÃ§Ãµes (Real-Time):** ConexÃ£o autenticada na API da SPTrans.
>
> **GTFS (EstÃ¡tico):** IngestÃ£o dos arquivos `.txt` contendo shapes, paradas e viagens.

### ğŸ¥ˆ Camada Silver (Limpeza e Modelagem)
> **NormalizaÃ§Ã£o:** Flatten de JSONs complexos.
>
> **Tipagem:** ConversÃ£o de coordenadas e timestamps.
>
> **DeduplicaÃ§Ã£o:** Garante unicidade dos registros de GPS.

### ğŸ¥‡ Camada Gold (InteligÃªncia)
> **CÃ¡lculo Geoespacial:** Uso da **FÃ³rmula de Haversine** para medir a extensÃ£o real das linhas (GTFS) e cruzar com a velocidade (GPS) para estimar o tempo de viagem.
>
> **HigienizaÃ§Ã£o:** Filtro de linhas fantasmas (velocidade sem frota ativa) para garantir precisÃ£o no dashboard.
<br>
> ## ğŸ“‚ Estrutura do RepositÃ³rio

```bash
sptrans-lakehouse/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py                     # ğŸ“Š Dashboard Streamlit + Chatbot
â”œâ”€â”€ databricks_notebooks/
â”‚   â”œâ”€â”€ ambiente/                  # ğŸ› ï¸ Setup de Schemas (RecuperaÃ§Ã£o de Metadados)
â”‚   â”œâ”€â”€ bronze/                    # ğŸ¥‰ IngestÃ£o API -> Delta Raw
â”‚   â”œâ”€â”€ silver/                    # ğŸ¥ˆ Tratamento e NormalizaÃ§Ã£o
â”‚   â””â”€â”€ gold/                      # ğŸ¥‡ KPIs e Regras de NegÃ³cio
â”œâ”€â”€ docs/                          # ğŸ“„ DocumentaÃ§Ã£o auxiliar
â”œâ”€â”€ requirements.txt               # ğŸ“¦ DependÃªncias Python
â””â”€â”€ README.md                      # ğŸ“˜ Este arquivo
