ðŸšŒ SPTrans Olho Vivo | Databricks Lakehouse (Standard + GTFS)
Pipeline de Engenharia de Dados End-to-End para monitoramento da frota de Ã´nibus de SÃ£o Paulo. Integra dados de telemetria em tempo real (API Olho Vivo) com dados estÃ¡ticos de planejamento (GTFS), processados em arquitetura Medallion (Bronze/Silver/Gold) no Databricks e visualizados em um Dashboard Streamlit com Chatbot.

ðŸ—ï¸ Arquitetura e Fluxo de Dados
O projeto foi desenhado para operar com eficiÃªncia de custos, utilizando recursos do plano Standard do Databricks na Azure.

```mermaid
graph TD
    %% DefiniÃ§Ã£o de Estilos
    classDef config fill:#f9f,stroke:#333,stroke-width:2px;
    classDef bronze fill:#cd7f32,stroke:#333,stroke-width:2px,color:white;
    classDef silver fill:#c0c0c0,stroke:#333,stroke-width:2px;
    classDef gold fill:#ffd700,stroke:#333,stroke-width:2px;
    classDef app fill:#61dafb,stroke:#333,stroke-width:2px;
    classDef source fill:#fff,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5;

    subgraph Fontes_Externas ["Fontes de Dados"]
        API_OlhoVivo["API Olho Vivo SPTrans"]:::source
        GTFS_Files["Arquivos GTFS EstÃ¡ticos"]:::source
    end

    subgraph Ambiente ["1. ConfiguraÃ§Ã£o do Ambiente"]
        Schemas["criacao_schemas.sql"]:::config
        Tab_Bronze["criacao_tabelas_bronze.sql"]:::config
        Tab_Silver["criacao_tabelas_silver.sql"]:::config
        Tab_Gold["criacao_tabelas_gold.sql"]:::config
        Tab_GTFS["criacao_tabelas_gtfs.py"]:::config
    end

    subgraph Bronze_Layer ["2. IngestÃ£o (Bronze)"]
        direction TB
        Ingest_Posicao["posicao_veiculos.py"]:::bronze
        Ingest_Linhas["buscar_linhas.py / buscar_linhas_sentido.py"]:::bronze
        Ingest_Paradas["buscar_paradas*.py"]:::bronze
        Ingest_Empresas["empresas.py / corredores.py"]:::bronze
        Ingest_Previsao["previsao_chegada*.py"]:::bronze
    end

    subgraph Silver_Layer ["3. Refinamento (Silver)"]
        direction TB
        Fato_Posicao["fato_posicao_veiculos.py"]:::silver
        Dim_Linhas["dim_linhas.py"]:::silver
        Dim_Empresas["dim_empresas.py"]:::silver
        Mapa_Shapes["mapa_shapes_gtfs.py"]:::silver
    end

    subgraph Gold_Layer ["4. AgregaÃ§Ã£o e KPIs (Gold)"]
        direction TB
        KPI_Velocidade["velocidade_linhas.py"]:::gold
        KPI_Snapshot["snapshot_frota_atual.py"]:::gold
        KPI_Acessibilidade["acessibilidade.py"]:::gold
        KPI_Rota["perfil_rota_estatico.sql"]:::gold
    end

    subgraph Visualization ["5. VisualizaÃ§Ã£o"]
        Streamlit_App["app/app.py"]:::app
    end

    %% ConexÃµes
    Schemas --> Tab_Bronze --> Tab_Silver --> Tab_Gold --> Tab_GTFS
    
    API_OlhoVivo --> Ingest_Posicao
    API_OlhoVivo --> Ingest_Linhas
    API_OlhoVivo --> Ingest_Paradas
    API_OlhoVivo --> Ingest_Empresas
    
    Tab_Bronze -.-> Ingest_Posicao
    
    Ingest_Posicao --> Fato_Posicao
    Ingest_Linhas --> Dim_Linhas
    Ingest_Empresas --> Dim_Empresas
    GTFS_Files --> Mapa_Shapes
    
    Fato_Posicao & Dim_Linhas --> KPI_Velocidade
    Fato_Posicao & Dim_Linhas & Dim_Empresas --> KPI_Snapshot
    Dim_Linhas & Fato_Posicao --> KPI_Acessibilidade
    Mapa_Shapes --> KPI_Rota
    
    KPI_Velocidade --> Streamlit_App
    KPI_Snapshot --> Streamlit_App
    KPI_Rota --> Streamlit_App
    KPI_Acessibilidade --> Streamlit_App
```


âš™ï¸ OrquestraÃ§Ã£o (Databricks Workflows)
A automaÃ§Ã£o do pipeline Ã© gerenciada nativamente pelo Databricks Workflows (Jobs), sem necessidade de ferramentas externas como Airflow.

Nome do Job: pipeline_olhovivo

FrequÃªncia: A cada 15 minutos (Cron Schedule).

Cluster: Cluster All-Purpose (Standard Mode).

Tasks do Workflow (ExecuÃ§Ã£o Sequencial):

1_ing_posic_veic_bronze: Conecta na API e baixa o JSON raw.

2_posic_veic_silver: Processa, explode e limpa os dados.

3_velocidade_gold: Calcula a mÃ©dia de velocidade e tempo de viagem.

4_snapshot_mapa: Atualiza a Ãºltima posiÃ§Ã£o conhecida da frota.


â˜ï¸ EstratÃ©gia de Infraestrutura e Custos (FinOps)
Este projeto adota uma arquitetura otimizada para reduzir custos de nuvem e licenciamento Databricks (DBUs), ideal para ambientes de desenvolvimento e POCs.

1. Armazenamento (Azure Storage vs. Catalog)
Dados FÃ­sicos (Parquet/Delta): Todos os dados persistem de forma segura em um Azure Storage Account (ADLS Gen2).

Metadados: Utilizamos o Hive Metastore (Legacy) embutido no cluster, ao invÃ©s do Unity Catalog, para evitar custos adicionais de gerenciamento e complexidade de setup em workspace Standard.

2. Metadados EfÃªmeros (Cluster-Scoped)
Como estratÃ©gia de economia, utilizamos o metastore local do cluster (banco Derby embutido).

Comportamento: Quando o cluster Ã© desligado/reiniciado, os ponteiros (schemas e definiÃ§Ãµes de tabelas) desaparecem da interface visual do Catalog.

PersistÃªncia: Os dados nÃ£o sÃ£o perdidos, pois estÃ£o salvos no Azure Storage.

RecuperaÃ§Ã£o: O pipeline inclui notebooks de "Ambiente" (criacao_schemas, criacao_tabelas) que recriam os ponteiros apontando para os locais existentes no Storage (LOCATION 'abfss://...') sempre que o ambiente Ã© reiniciado.


ðŸ§  LÃ³gica de NegÃ³cio (Camadas)
1. Camada Bronze (IngestÃ£o Raw)
PosiÃ§Ãµes (Real-Time): ConexÃ£o autenticada na API da SPTrans.

GTFS (EstÃ¡tico): IngestÃ£o dos arquivos .txt contendo shapes, paradas e viagens.

2. Camada Silver (Limpeza e Modelagem)
NormalizaÃ§Ã£o: Flatten de JSONs complexos.

Tipagem: ConversÃ£o de coordenadas e timestamps.

DeduplicaÃ§Ã£o: Garante unicidade dos registros de GPS.

3. Camada Gold (InteligÃªncia)
CÃ¡lculo Geoespacial: Uso da FÃ³rmula de Haversine para medir a extensÃ£o real das linhas (GTFS) e cruzar com a velocidade (GPS) para estimar o tempo de viagem.

HigienizaÃ§Ã£o: Filtro de linhas fantasmas (velocidade sem frota ativa) para garantir precisÃ£o no dashboard.


ðŸ“‚ Estrutura do RepositÃ³rio

sptrans-lakehouse/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py                     # Dashboard Streamlit + Chatbot
â”œâ”€â”€ databricks_notebooks/
â”‚   â”œâ”€â”€ ambiente/                  # Setup de Schemas (RecuperaÃ§Ã£o de Metadados)
â”‚   â”œâ”€â”€ bronze/                    # IngestÃ£o API -> Delta Raw
â”‚   â”œâ”€â”€ silver/                    # Tratamento e NormalizaÃ§Ã£o
â”‚   â””â”€â”€ gold/                      # KPIs e Regras de NegÃ³cio
â”œâ”€â”€ docs/                          # DocumentaÃ§Ã£o auxiliar
â”œâ”€â”€ requirements.txt               # DependÃªncias Python
â””â”€â”€ README.md                      # Este arquivo


ðŸš€ Como Executar
Passo a Passo
Setup no Databricks:

Importe os notebooks da pasta databricks_notebooks/.

Execute os scripts da pasta ambiente/ para montar os Schemas no Hive Metastore.

Nota: Se o cluster reiniciar, execute estes scripts novamente para restaurar a visibilidade das tabelas.

Agendamento:

Crie um Job apontando para os notebooks na ordem descrita na seÃ§Ã£o "OrquestraÃ§Ã£o".

VisualizaÃ§Ã£o (Local):

Configure suas credenciais no .streamlit/secrets.toml.

Execute o comando:

streamlit run app/app.py


Desenvolvido como parte do portfÃ³lio de Engenharia de Dados com foco em Databricks e Azure.
