# Databricks notebook source
# MAGIC %md
# MAGIC Realiza uma busca das linhas do sistema com base no par√¢metro informado. Se a linha n√£o √© encontrada ent√£o √© realizada uma busca fonetizada na denomina√ß√£o das linhas. A linha retornada ser√° unicamente aquela cujo sentido de opera√ß√£o seja o informado no par√¢metro sentido

# COMMAND ----------

# ============================================================
# Ingest√£o incremental da API Olho Vivo - Linhas por Sentido
# Frequ√™ncia: a cada 15 minutos
# Camada: Bronze (Azure Standard / Hive Metastore)
# ============================================================

import requests
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, lit
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType
from datetime import datetime

# --- 1. CONFIGURA√á√ïES ---
storage_account = "datalakeprojmba" 
container = "datalakeprojmba" 
meu_token = "6af34791de77cb4fe108edab6287fa51829405c1781e99e0e2d81cd0114cf3e7"

# Caminhos
base_path = f"abfss://{container}@{storage_account}.dfs.core.windows.net"
caminho_landing = f"{base_path}/olhovivo/bronze/landing_zone/linhas_sentido"
tabela_full_name = "olhovivo_bronze.buscar_linhas_sentido"
base_url = "https://api.olhovivo.sptrans.com.br/v2.1"

# --- LIMPEZA DE AMBIENTE (DEV) ---
# Executa a faxina para garantir que recriaremos a tabela sem conflitos de metadados
caminho_tabela_fisico = f"{base_path}/olhovivo/bronze/buscar_linhas_sentido"

# Descomente as linhas abaixo se precisar "zerar" a tabela
dbutils.fs.rm(caminho_tabela_fisico, True)
spark.sql(f"DROP TABLE IF EXISTS {tabela_full_name}")
print(f"üßπ Faxina completa em: {caminho_tabela_fisico}")


# --- 2. DEFINI√á√ÉO EXPL√çCITA DO SCHEMA ---
# Essencial para evitar erro de INT vs BIGINT
schema_linhas_sentido = StructType([
    StructField("cl", IntegerType(), True), 
    StructField("lc", BooleanType(), True), 
    StructField("lt", StringType(), True),  
    StructField("sl", IntegerType(), True), 
    StructField("tl", IntegerType(), True), 
    StructField("tp", StringType(), True),  
    StructField("ts", StringType(), True)   
])

# --- 3. AUTENTICA√á√ÉO ---
session = requests.Session()
auth = session.post(f"{base_url}/Login/Autenticar?token={meu_token}")

if auth.text.lower() == "true":
    print("‚úÖ Autenticado com sucesso na API Olho Vivo")
else:
    raise Exception(f"‚ùå Falha na autentica√ß√£o: {auth.text}")

# --- 4. COLETA DE DADOS (LOOP) ---
lista_dfs = []
total_consultas = 0
consultas_sucesso = 0

print("üîç Iniciando varredura (Linhas 1-9 x Sentidos 1-2)...")

for codigo_linha in range(1, 10):
    for sentido in range(1, 3):
        total_consultas += 1
        # url = f"{base_url}/Linha/BuscarLinhaSentido?termosBusca={codigo_linha}&sentido={sentido}"
        
        try:
            # Usando params do requests fica mais limpo e seguro
            resp = session.get(
                f"{base_url}/Linha/BuscarLinhaSentido", 
                params={"termosBusca": codigo_linha, "sentido": sentido},
                timeout=10
            )
            
            if resp.status_code == 200:
                dados = resp.json()
                
                if len(dados) > 0:
                    consultas_sucesso += 1
                    # APLICANDO O SCHEMA AQUI PARA EVITAR ERRO DE TIPO
                    df_temp = spark.createDataFrame(dados, schema=schema_linhas_sentido)
                    lista_dfs.append(df_temp)
            else:
                print(f"   ‚ö†Ô∏è Erro HTTP {resp.status_code} no termo {codigo_linha}/{sentido}")
                
        except Exception as e:
            print(f"   ‚ùå Erro na consulta {codigo_linha}/{sentido}: {str(e)}")
            continue

print("="*30)
print(f"Resumo: {consultas_sucesso} sucessos em {total_consultas} tentativas.")
print("="*30)

# --- 5. PROCESSAMENTO E GRAVA√á√ÉO ---
if lista_dfs:
    # Uni√£o
    df = lista_dfs[0]
    for df_temp in lista_dfs[1:]:
        df = df.union(df_temp)

    # Enriquecimento
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    df = df.withColumn("dt_ingestao", current_timestamp()) \
           .withColumn("arquivo_origem", lit(f"linhas_sentido_{timestamp_str}.json"))
           
    print(f"‚úÖ Total de registros combinados: {df.count()}")

    # A. Salva JSON Raw (Landing Zone)
    caminho_arquivo_json = f"{caminho_landing}/linhas_sentido_{timestamp_str}.json"
    df.write.mode("overwrite").json(caminho_arquivo_json)
    print(f"üóÇÔ∏è JSON salvo em: {caminho_arquivo_json}")

    # B. Salva Delta (Bronze)
    # Cria√ß√£o da tabela com tipagem correta
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {tabela_full_name} (
            cl INT COMMENT 'C√≥digo identificador da linha. √önico por linha e sentido.',
            lc BOOLEAN COMMENT 'Indica se a linha √© circular (sem terminal secund√°rio).',
            lt STRING COMMENT 'Primeira parte do letreiro num√©rico da linha.',
            sl INT COMMENT 'Sentido da linha: 1 = Terminal Principal ‚Üí Terminal Secund√°rio; 2 = inverso.',
            tl INT COMMENT 'Letreiro num√©rico (Parte 2).',
            tp STRING COMMENT 'Letreiro descritivo da linha no sentido Terminal Principal ‚Üí Terminal Secund√°rio.',
            ts STRING COMMENT 'Letreiro descritivo da linha no sentido Terminal Secund√°rio ‚Üí Terminal Principal.',
            dt_ingestao TIMESTAMP COMMENT 'Timestamp da ingest√£o dos dados no ambiente Bronze.',
            arquivo_origem STRING COMMENT 'Nome do arquivo JSON que originou os dados.'
        ) USING DELTA
    """)

    # Append
    (df.write
       .format("delta")
       .mode("append")
       .option("mergeSchema", "true") 
       .saveAsTable(tabela_full_name)
    )

    print(f"‚úÖ Sucesso! Tabela '{tabela_full_name}' atualizada.")

    # C. Metadados e Coment√°rios (Documentation)
    descricao = "Tabela Bronze: Linhas filtradas por Sentido (1 ou 2)."
    spark.sql(f"COMMENT ON TABLE {tabela_full_name} IS '{descricao}'")
    
    # Estat√≠sticas R√°pidas
    print("\nDistribui√ß√£o por Sentido:")
    df.groupBy("sl").count().show()
    
    display(df.limit(5))

else:
    print("‚ùå Nenhum dado encontrado. Verifique se a API est√° online.")