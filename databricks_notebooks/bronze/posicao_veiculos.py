# Databricks notebook source
# MAGIC %md
# MAGIC A categoria Posi√ß√£o Dos Ve√≠culos √© a respons√°vel por retornar a posi√ß√£o exata de cada ve√≠culo de qualquer linha de √¥nibus da SPTrans

# COMMAND ----------

# ============================================================
# Ingest√£o incremental da API Olho Vivo - Posi√ß√£o (FATO)
# Frequ√™ncia: A cada 5, 10 ou 15 minutos
# Camada: Bronze (Azure Standard / Hive Metastore)
# Vers√£o: Com Retry e Tratamento de Erros
# ============================================================

import requests
import json
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, lit
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType
from datetime import datetime

# --- 0. FUN√á√ïES AUXILIARES (RETRY LOGIC) ---
def buscar_dados_com_retry(session, url, tentativas=3):
    """
    Tenta buscar dados na API. Se falhar, espera e tenta de novo.
    Retorna o JSON dos dados ou None se falhar todas as vezes.
    """
    for i in range(tentativas):
        try:
            print(f"üîÑ Tentativa {i+1} de {tentativas}...")
            response = session.get(url, timeout=30) # Timeout aumentado para 30s
            
            if response.status_code == 200:
                dados = response.json()
                if dados: # Verifica se n√£o veio vazio
                    return dados
                else:
                    print("‚ö†Ô∏è API retornou 200 OK mas o conte√∫do estava vazio.")
            else:
                print(f"‚ö†Ô∏è Erro HTTP: {response.status_code}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erro de conex√£o na tentativa {i+1}: {e}")
        
        # Espera 5 segundos antes da pr√≥xima tentativa (Backoff)
        time.sleep(5)
    
    print("‚ùå Falha total ap√≥s todas as tentativas.")
    return None

# --- 1. CONFIGURA√á√ïES ---
storage_account = "datalakeprojmba" 
container = "datalakeprojmba" 
meu_token = "6af34791de77cb4fe108edab6287fa51829405c1781e99e0e2d81cd0114cf3e7"

# Caminhos
base_path = f"abfss://{container}@{storage_account}.dfs.core.windows.net"
caminho_landing = f"{base_path}/olhovivo/bronze/landing_zone/posicao"
tabela_full_name = "olhovivo_bronze.posicao_veiculos"
base_url = "https://api.olhovivo.sptrans.com.br/v2.1"

# --- 2. DEFINI√á√ÉO EXPL√çCITA DO SCHEMA (OUTPUT) ---
schema_final = StructType([
    StructField("hr", StringType(), True),  
    StructField("cl", IntegerType(), True), 
    StructField("c", StringType(), True),   
    StructField("sl", IntegerType(), True), 
    StructField("lt1", StringType(), True), 
    StructField("lt0", StringType(), True), 
    StructField("p", StringType(), True),   
    StructField("a", BooleanType(), True),  
    StructField("ta", StringType(), True),  
    StructField("py", DoubleType(), True),  
    StructField("px", DoubleType(), True)   
])

# --- 3. AUTENTICA√á√ÉO ---
session = requests.Session()
try:
    auth = session.post(f"{base_url}/Login/Autenticar?token={meu_token}")
    if auth.text.lower() != "true":
        raise Exception(f"‚ùå Falha na autentica√ß√£o: {auth.text}")
    print("‚úÖ Autenticado com sucesso.")
except Exception as e:
    print(f"‚ùå Erro fatal na autentica√ß√£o: {e}")
    dbutils.notebook.exit("Falha Auth") # Encerra o notebook se n√£o autenticar

# --- 4. COLETA E LANDING (COM RETRY) ---
print("üîç Buscando posi√ß√£o global dos ve√≠culos...")

# >>> AQUI EST√Å A MUDAN√áA PRINCIPAL <<<
# Usamos a fun√ß√£o criada l√° em cima em vez de chamar direto
raw_data = buscar_dados_com_retry(session, f"{base_url}/Posicao")

df = None # Inicializa vari√°vel

if raw_data:
    hora_ref = raw_data.get("hr", "00:00")
    
    # --- A. GRAVA√á√ÉO NA LANDING ZONE (SEGURAN√áA) ---
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    caminho_arquivo_json = f"{caminho_landing}/posicao_{timestamp_str}.json"
    
    try:
        dbutils.fs.put(caminho_arquivo_json, json.dumps(raw_data), overwrite=True)
        print(f"üóÇÔ∏è JSON bruto salvo em: {caminho_arquivo_json}")
    except Exception as e:
        print(f"‚ö†Ô∏è Aviso: N√£o foi poss√≠vel salvar na Landing Zone, mas seguindo fluxo: {e}")

    # --- B. TRANSFORMA√á√ÉO (FLATTENING) ---
    lista_veiculos = []
    
    # O .get() aqui √© seguro porque j√° checamos 'if raw_data'
    linhas_api = raw_data.get("l", [])
    
    if linhas_api:
        print(f"üîπ Processando {len(linhas_api)} linhas operacionais...")

        for linha in linhas_api:
            # Prote√ß√£o extra: verifica se 'linha' n√£o √© None
            if linha: 
                veiculos = linha.get("vs", [])
                for v in veiculos:
                    if v: # Prote√ß√£o extra para ve√≠culo
                        lista_veiculos.append({
                            "hr": hora_ref,
                            "cl": linha.get("cl"),
                            "c": linha.get("c"),
                            "sl": linha.get("sl"),
                            "lt1": linha.get("lt1"),
                            "lt0": linha.get("lt0"),
                            "p": v.get("p"),
                            "a": v.get("a"),
                            "ta": v.get("ta"),
                            "py": v.get("py"),
                            "px": v.get("px")
                        })
        
        print(f"‚úÖ Total de ve√≠culos mapeados: {len(lista_veiculos)}")

        if lista_veiculos:
            df = spark.createDataFrame(lista_veiculos, schema=schema_final)
        else:
            print("‚ö†Ô∏è Lista de ve√≠culos vazia ap√≥s processamento.")
    else:
        print("‚ö†Ô∏è Objeto 'l' (linhas) veio vazio da API.")

else:
    # Se raw_data for None (falhou as 3 tentativas)
    print("‚ùå Abortando: N√£o foi poss√≠vel obter dados da API ap√≥s m√∫ltiplas tentativas.")
    # Opcional: Encerrar com erro para o Job saber que falhou
    # dbutils.notebook.exit("Falha na Coleta")

# --- 5. GRAVA√á√ÉO DELTA ---
if df:
    # Enriquecimento
    df = df.withColumn("dt_ingestao", current_timestamp()) \
           .withColumn("arquivo_origem", lit(f"posicao_{timestamp_str}.json"))

    # Cria√ß√£o da Tabela (DDL)
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {tabela_full_name} (
            hr STRING COMMENT 'Hor√°rio de refer√™ncia do conjunto de dados retornado pela API.',
            cl INT COMMENT 'C√≥digo identificador da linha.',
            c STRING COMMENT 'Letreiro completo da linha (exemplo: "1012-10").',
            sl INT COMMENT 'Sentido da linha: 1 = Terminal Principal ‚Üí Terminal Secund√°rio; 2 = inverso.',
            lt1 STRING COMMENT 'Nome do Terminal Principal da linha.',
            lt0 STRING COMMENT 'Nome do Terminal Secund√°rio da linha.',
            p STRING COMMENT 'Prefixo do ve√≠culo (identificador √∫nico do √¥nibus).',
            a BOOLEAN COMMENT 'Indica se o ve√≠culo √© acess√≠vel (PNE).',
            ta STRING COMMENT 'Timestamp UTC da √∫ltima atualiza√ß√£o da posi√ß√£o do ve√≠culo.',
            py DOUBLE COMMENT 'Latitude da posi√ß√£o atual do ve√≠culo.',
            px DOUBLE COMMENT 'Longitude da posi√ß√£o atual do ve√≠culo.',
            dt_ingestao TIMESTAMP COMMENT 'Timestamp da ingest√£o dos dados no ambiente Bronze.',
            arquivo_origem STRING COMMENT 'Nome do arquivo JSON que originou os dados.'
        ) USING DELTA
    """)

    # Append
    (df.write
       .format("delta")
       .mode("append")
       .option("mergeSchema", "true") 
       .saveAsTable(tabela_full_name)
    )

    print(f"‚úÖ Sucesso! Tabela '{tabela_full_name}' atualizada.")
    
    # Resumo
    print("Distribui√ß√£o por Sentido:")
    df.groupBy("sl").count().show()

else:
    print("‚ö†Ô∏è Pipeline finalizado sem grava√ß√£o (dados vazios ou erro na coleta).")