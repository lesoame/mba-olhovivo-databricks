# Databricks notebook source
# ============================================================
# Ingest√£o incremental: Previs√£o de Chegada (Baseada no GTFS)
# Frequ√™ncia: Alta frequ√™ncia (Real-time)
# Camada: Bronze (Azure Standard / Hive Metastore)
# ============================================================

import requests
import time
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, lit, col
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType
from datetime import datetime

# --- 1. CONFIGURA√á√ïES ---
storage_account = "datalakeprojmba" 
container = "datalakeprojmba" 
meu_token = "6af34791de77cb4fe108edab6287fa51829405c1781e99e0e2d81cd0114cf3e7"

# Caminhos
base_path = f"abfss://{container}@{storage_account}.dfs.core.windows.net"
caminho_landing = f"{base_path}/olhovivo/bronze/landing_zone/previsao_parada"
tabela_full_name = "olhovivo_bronze.previsao_chegada_parada"
base_url = "https://api.olhovivo.sptrans.com.br/v2.1"

# Tamanho do lote para salvar no Delta (evita perder tudo se der erro)
BATCH_SIZE = 500 

# --- LIMPEZA DE AMBIENTE (DEV) ---
# Se quiser zerar a tabela para come√ßar limpo com o GTFS:
# dbutils.fs.rm(f"{base_path}/olhovivo/bronze/previsao_chegada_parada", True)
# spark.sql(f"DROP TABLE IF EXISTS {tabela_full_name}")


# --- 2. DEFINI√á√ÉO DO SCHEMA ---
schema_previsao = StructType([
    StructField("hr", StringType(), True),
    StructField("cp", IntegerType(), True),
    StructField("np", StringType(), True),
    StructField("py_parada", DoubleType(), True),
    StructField("px_parada", DoubleType(), True),
    StructField("c", StringType(), True),
    StructField("cl", IntegerType(), True),
    StructField("sl", IntegerType(), True),
    StructField("lt0", StringType(), True),
    StructField("lt1", StringType(), True),
    StructField("qv", IntegerType(), True),
    StructField("p", StringType(), True),
    StructField("t", StringType(), True),
    StructField("a", BooleanType(), True),
    StructField("ta", StringType(), True),
    StructField("py", DoubleType(), True),
    StructField("px", DoubleType(), True)
])

# --- 3. AUTENTICA√á√ÉO ---
session = requests.Session()
auth = session.post(f"{base_url}/Login/Autenticar?token={meu_token}")
if auth.text.lower() != "true":
    raise Exception(f"‚ùå Falha na autentica√ß√£o: {auth.text}")
print("‚úÖ Autenticado.")


# --- 4. PREPARA√á√ÉO: LER PARADAS DO GTFS (MUDAN√áA AQUI) ---
print("üîç Lendo cadastro mestre de paradas (GTFS)...")

try:
    # Lemos a tabela GTFS que criamos
    df_gtfs = spark.table("olhovivo_bronze.gtfs_stops")
    
    # Selecionamos os IDs (stop_id). 
    # O GTFS trata como String, a API precisa de Int.
    # Filtramos apenas os num√©ricos para evitar erros.
    df_ids = df_gtfs.select("stop_id").distinct()
    
    # ‚ö†Ô∏è LIMITADOR PARA O MBA ‚ö†Ô∏è
    # O GTFS tem ~20.000 paradas. Consultar todas leva ~1 hora num loop simples.
    # Para o projeto, vamos pegar uma amostra de 100 paradas para provar o conceito.
    # Se quiser todas, remova o .limit(100).
    lista_rows = df_ids.collect()
    
    lista_ids = []
    for row in lista_rows:
        try:
            # Converte para int, pois a API espera n√∫mero
            lista_ids.append(int(row['stop_id']))
        except:
            continue
            
    print(f"üîπ Paradas do GTFS carregadas para consulta: {len(lista_ids)}")

except Exception as e:
    raise Exception(f"Erro ao ler olhovivo_bronze.gtfs_stops. Verifique se rodou a ingest√£o GTFS! {e}")


# --- 5. LOOP DE PROCESSAMENTO (BATCH) ---
total_registros_geral = 0
timestamp_geral = datetime.now().strftime("%Y%m%d_%H%M%S")

# Fun√ß√£o Helper
def achatar_json(dados_json):
    flat_list = []
    if not dados_json: return flat_list
    hr = dados_json.get("hr")
    p_root = dados_json.get("p")
    if not p_root: return flat_list
    paradas = [p_root] if isinstance(p_root, dict) else p_root

    for p in paradas:
        cp = p.get("cp")
        np_name = p.get("np")
        py_par = p.get("py")
        px_par = p.get("px")
        linhas = p.get("l", [])
        if not linhas: continue

        for l in linhas:
            vs = l.get("vs", [])
            for v in vs:
                flat_list.append({
                    "hr": hr, "cp": cp, "np": np_name, "py_parada": py_par, "px_parada": px_par,
                    "c": l.get("c"), "cl": l.get("cl"), "sl": l.get("sl"),
                    "lt0": l.get("lt0"), "lt1": l.get("lt1"), "qv": l.get("qv"),
                    "p": str(v.get("p")), "t": v.get("t"), "a": v.get("a"),
                    "ta": v.get("ta"), "py": v.get("py"), "px": v.get("px")
                })
    return flat_list

# Execu√ß√£o do Loop
for i in range(0, len(lista_ids), BATCH_SIZE):
    lote_ids = lista_ids[i:i+BATCH_SIZE]
    print(f"\nüöÄ Processando lote {i//BATCH_SIZE + 1} ({len(lote_ids)} paradas)...")
    
    registros_lote = []
    
    for pid in lote_ids:
        try:
            resp = session.get(f"{base_url}/Previsao/Parada?codigoParada={pid}", timeout=5)
            if resp.status_code == 200:
                registros_lote.extend(achatar_json(resp.json()))
        except:
            continue # Se falhar uma parada, segue o baile
    
    # Grava se tiver dados
    if registros_lote:
        df_lote = spark.createDataFrame(registros_lote, schema=schema_previsao)
        
        # Enriquecimento
        df_lote = df_lote.withColumn("dt_ingestao", current_timestamp()) \
                         .withColumn("arquivo_origem", lit(f"gtfs_batch_{i}_{timestamp_geral}.json"))
        
        qtd = df_lote.count()
        total_registros_geral += qtd

        # Salva JSON Raw (Landing)
        caminho_raw = f"{caminho_landing}/gtfs_batch_{i}_{timestamp_geral}.json"
        df_lote.write.mode("overwrite").json(caminho_raw)

        # Salva Delta (Bronze)
        # Recria tabela se n√£o existir
        spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {tabela_full_name} (
                hr STRING COMMENT 'Hor√°rio de refer√™ncia da gera√ß√£o das informa√ß√µes.', 
                cp INT COMMENT 'C√≥digo identificador da parada.', 
                np STRING COMMENT 'Nome da parada.', 
                py_parada DOUBLE COMMENT 'Informa√ß√£o de latitude da localiza√ß√£o do ve√≠culo.', 
                px_parada DOUBLE COMMENT 'Informa√ß√£o de longitude da localiza√ß√£o do ve√≠culo.',
                c STRING COMMENT 'Letreiro completo.', 
                cl INT COMMENT 'C√≥digo identificador da linha.', 
                sl INT COMMENT 'Sentido de opera√ß√£o onde 1 significa de Terminal Principal para Terminal Secund√°rio e 2 de Terminal Secund√°rio para Terminal Principal.', 
                lt0 STRING COMMENT 'Letreiro de destino da linha.', 
                lt1 STRING COMMENT 'Letreiro de origem da linha.', 
                qv INT COMMENT 'Quantidade de ve√≠culos localizados.',
                p STRING COMMENT 'Prefixo do ve√≠culo.', 
                t STRING COMMENT 'Hor√°rio previsto para chegada do ve√≠culo no ponto de parada relacionado.', 
                a BOOLEAN COMMENT 'Indica se o ve√≠culo √© (true) ou n√£o (false) acess√≠vel para pessoas com defici√™ncia.', 
                ta STRING COMMENT 'Indica o hor√°rio universal (UTC) em que a localiza√ß√£o foi capturada. Essa informa√ß√£o est√° no padr√£o ISO 8601.', 
                py DOUBLE COMMENT 'Informa√ß√£o de latitude da localiza√ß√£o do ve√≠culo.', 
                px DOUBLE COMMENT 'Informa√ß√£o de longitude da localiza√ß√£o do ve√≠culo.',
                dt_ingestao TIMESTAMP COMMENT 'Timestamp da ingest√£o dos dados no ambiente Bronze.', 
                arquivo_origem STRING COMMENT 'Nome do arquivo JSON que originou os dados.'
            ) USING DELTA
        """)
        
        (df_lote.write
           .format("delta")
           .mode("append")
           .option("mergeSchema", "true") 
           .saveAsTable(tabela_full_name)
        )
        print(f"   ‚úÖ {qtd} previs√µes salvas.")
    else:
        print("   ‚ö†Ô∏è Nenhuma previs√£o (√¥nibus chegando) encontrada neste lote.")

print(f"\nüèÅ Processamento GTFS finalizado. Total: {total_registros_geral}")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(*) FROM olhovivo_bronze.previsao_chegada_parada;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM olhovivo_bronze.previsao_chegada_parada LIMIT 10;

# COMMAND ----------

# ============================================================
# Atualiza√ß√£o de Metadados (Coment√°rios)
# Tabela: olhovivo_bronze.previsao_chegada_parada
# ============================================================

tabela_alvo = "olhovivo_bronze.previsao_chegada_parada"

# Dicion√°rio com Coluna -> Descri√ß√£o
comentarios_colunas = {
    "hr": "Hor√°rio de refer√™ncia da gera√ß√£o das informa√ß√µes.",
    "cp": "C√≥digo identificador da parada.",
    "np": "Nome da parada.",
    "py_parada": "Latitude da localiza√ß√£o da parada.", 
    "px_parada": "Longitude da localiza√ß√£o da parada.",
    "c": "Letreiro completo.",
    "cl": "C√≥digo identificador da linha.",
    "sl": "Sentido de opera√ß√£o onde 1 significa de Terminal Principal para Terminal Secund√°rio e 2 de Terminal Secund√°rio para Terminal Principal.",
    "lt0": "Letreiro de destino da linha.",
    "lt1": "Letreiro de origem da linha.",
    "qv": "Quantidade de ve√≠culos localizados.",
    "p": "Prefixo do ve√≠culo.",
    "t": "Hor√°rio previsto para chegada do ve√≠culo no ponto de parada relacionado.",
    "a": "Indica se o ve√≠culo √© (true) ou n√£o (false) acess√≠vel para pessoas com defici√™ncia.",
    "ta": "Indica o hor√°rio universal (UTC) em que a localiza√ß√£o foi capturada. Essa informa√ß√£o est√° no padr√£o ISO 8601.",
    "py": "Informa√ß√£o de latitude da localiza√ß√£o do ve√≠culo.",
    "px": "Informa√ß√£o de longitude da localiza√ß√£o do ve√≠culo.",
    "dt_ingestao": "Timestamp da ingest√£o dos dados no ambiente Bronze.",
    "arquivo_origem": "Nome do arquivo JSON que originou os dados."
}

print(f"üîÑ Iniciando atualiza√ß√£o de coment√°rios na tabela: {tabela_alvo}...")

# 1. Atualiza descri√ß√£o da tabela
spark.sql(f"COMMENT ON TABLE {tabela_alvo} IS 'Tabela Bronze: Previs√£o de chegada dos ve√≠culos nas paradas (Origem: API Olho Vivo).'")

# 2. Atualiza colunas uma por uma
for coluna, comentario in comentarios_colunas.items():
    try:
        # Comando SQL para alterar apenas o coment√°rio da coluna
        spark.sql(f"ALTER TABLE {tabela_alvo} ALTER COLUMN {coluna} COMMENT '{comentario}'")
        print(f"   ‚úÖ Coluna '{coluna}' documentada.")
    except Exception as e:
        print(f"   ‚ö†Ô∏è Erro na coluna '{coluna}': {e}")

print("\nüèÅ Atualiza√ß√£o de metadados conclu√≠da!")

# 3. Valida√ß√£o: Mostra como ficou
display(spark.sql(f"DESCRIBE TABLE EXTENDED {tabela_alvo}"))