# Databricks notebook source
# MAGIC %md
# MAGIC Retorna uma lista com a previs√£o de chegada de cada um dos ve√≠culos da linha informada em todos os pontos de parada aos quais que ela atende.

# COMMAND ----------

# ============================================================
# Ingest√£o incremental: Previs√£o de Chegada (Endpoint /Previsao/Linha)
# Frequ√™ncia: Alta frequ√™ncia (Real-time)
# Camada: Bronze (Azure Standard / Hive Metastore)
# ============================================================

import requests
import time
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, lit
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType
from datetime import datetime

# --- 1. CONFIGURA√á√ïES ---
storage_account = "datalakeprojmba" 
container = "datalakeprojmba" 
meu_token = "6af34791de77cb4fe108edab6287fa51829405c1781e99e0e2d81cd0114cf3e7"

# Caminhos
base_path = f"abfss://{container}@{storage_account}.dfs.core.windows.net"
caminho_landing = f"{base_path}/olhovivo/bronze/landing_zone/previsao_linha"
tabela_full_name = "olhovivo_bronze.previsao_chegada_linha"
base_url = "https://api.olhovivo.sptrans.com.br/v2.1"

# Batch Size menor pois a resposta desse endpoint √© GRANDE (traz todas as paradas da linha)
BATCH_SIZE = 20 

# --- LIMPEZA DE AMBIENTE (DEV) ---
# dbutils.fs.rm(f"{base_path}/olhovivo/bronze/previsao_chegada_linha", True)
#spark.sql(f"DROP TABLE IF EXISTS {tabela_full_name}")


# --- 2. DEFINI√á√ÉO DO SCHEMA (FLATTENED) ---
schema_previsao_linha = StructType([
    StructField("hr", StringType(), True),      
    StructField("cod_linha", IntegerType(), True), 
    StructField("cp", IntegerType(), True),     
    StructField("np", StringType(), True),      
    StructField("py_parada", DoubleType(), True), 
    StructField("px_parada", DoubleType(), True), 
    StructField("p", StringType(), True),       
    StructField("t", StringType(), True),       
    StructField("a", BooleanType(), True),      
    StructField("ta", StringType(), True),      
    StructField("py", DoubleType(), True),      
    StructField("px", DoubleType(), True)       
])

# --- 3. AUTENTICA√á√ÉO ---
session = requests.Session()
auth = session.post(f"{base_url}/Login/Autenticar?token={meu_token}")
if auth.text.lower() != "true":
    raise Exception(f"‚ùå Falha na autentica√ß√£o: {auth.text}")
print("‚úÖ Autenticado.")


# --- 4. PREPARA√á√ÉO: OBTER LISTA DE LINHAS ---
print("üîç Lendo lista de linhas ativas (olhovivo_bronze.buscar_linhas)...")
try:
    # Usamos a tabela que j√° ingerimos para saber quais linhas consultar
    df_linhas = spark.table("olhovivo_bronze.buscar_linhas")
    
    # ‚ö†Ô∏è LIMITADOR DE SEGURAN√áA (MBA) ‚ö†Ô∏è
    # Selecionamos 50 linhas aleat√≥rias para teste.
    # Remova o .limit(50) para produ√ß√£o, mas vai demorar bastante.
    lista_codigos = [row['cl'] for row in df_linhas.select('cl').distinct().limit(50).collect()]
    
    print(f"üîπ Linhas selecionadas para consulta: {len(lista_codigos)}")

except Exception as e:
    raise Exception(f"Erro ao ler tabela de linhas: {e}")


# --- 5. LOOP DE PROCESSAMENTO ---
total_registros_geral = 0
timestamp_geral = datetime.now().strftime("%Y%m%d_%H%M%S")

def achatar_json_linha(dados_json, codigo_linha):
    flat_list = []
    if not dados_json: return flat_list
    
    hr = dados_json.get("hr")
    # A API retorna um objeto "ps" (paradas) que cont√©m uma lista
    ps_root = dados_json.get("ps") 
    
    if not ps_root: return flat_list
    paradas = [ps_root] if isinstance(ps_root, dict) else ps_root

    for p in paradas:
        # Dados da Parada
        cp = p.get("cp")
        np_name = p.get("np")
        py_par = p.get("py")
        px_par = p.get("px")
        
        # Lista de Ve√≠culos vindo para esta parada
        vs = p.get("vs", [])
        if not vs: continue

        for v in vs:
            flat_list.append({
                "hr": hr,
                "cod_linha": codigo_linha, # Importante para rastreio
                "cp": cp,
                "np": np_name,
                "py_parada": py_par,
                "px_parada": px_par,
                "p": str(v.get("p")), 
                "t": v.get("t"),
                "a": v.get("a"),
                "ta": v.get("ta"),
                "py": v.get("py"),
                "px": v.get("px")
            })
    return flat_list

# Loop Principal
for i in range(0, len(lista_codigos), BATCH_SIZE):
    lote_cods = lista_codigos[i:i+BATCH_SIZE]
    print(f"\nüöÄ Processando lote {i//BATCH_SIZE + 1} ({len(lote_cods)} linhas)...")
    
    registros_lote = []
    
    for cl in lote_cods:
        try:
            resp = session.get(f"{base_url}/Previsao/Linha?codigoLinha={cl}", timeout=10)
            if resp.status_code == 200:
                registros_lote.extend(achatar_json_linha(resp.json(), cl))
            else:
                # Erro 404 √© comum se a linha n√£o estiver operando agora
                pass 
        except Exception as e:
            print(f"   Erro na linha {cl}: {e}")
            continue
    
    # Grava√ß√£o
    if registros_lote:
        df_lote = spark.createDataFrame(registros_lote, schema=schema_previsao_linha)
        
        # Enriquecimento
        df_lote = df_lote.withColumn("dt_ingestao", current_timestamp()) \
                         .withColumn("arquivo_origem", lit(f"batch_{i}_{timestamp_geral}.json"))
        
        qtd = df_lote.count()
        total_registros_geral += qtd

        # Salva Raw e Delta
        caminho_raw = f"{caminho_landing}/batch_{i}_{timestamp_geral}.json"
        df_lote.write.mode("overwrite").json(caminho_raw)

        spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {tabela_full_name} (
                hr STRING COMMENT 'Hor√°rio de refer√™ncia da gera√ß√£o das informa√ß√µes.', 
                cod_linha INT COMMENT 'C√≥digo identificador da linha de √¥nibus consultada (FK).', 
                cp INT COMMENT 'C√≥digo identificador da parada.', 
                np STRING COMMENT 'Nome da parada.', 
                py_parada DOUBLE COMMENT 'Informa√ß√£o de latitude da localiza√ß√£o do ve√≠culo.', 
                px_parada DOUBLE COMMENT 'Informa√ß√£o de longitude da localiza√ß√£o do ve√≠culo.', 
                p STRING COMMENT 'Prefixo do ve√≠culo.', 
                t STRING COMMENT 'Hor√°rio previsto para chegada do ve√≠culo no ponto de parada relacionado.', 
                a BOOLEAN COMMENT 'Indica se o ve√≠culo √© (true) ou n√£o (false) acess√≠vel para pessoas com defici√™ncia.', 
                ta STRING COMMENT 'Indica o hor√°rio universal (UTC) em que a localiza√ß√£o foi capturada. Essa informa√ß√£o est√° no padr√£o ISO 8601.', 
                py DOUBLE COMMENT 'Informa√ß√£o de latitude da localiza√ß√£o do ve√≠culo.', 
                px DOUBLE COMMENT 'Informa√ß√£o de longitude da localiza√ß√£o do ve√≠culo.',
                dt_ingestao TIMESTAMP COMMENT 'Timestamp da ingest√£o dos dados no ambiente Bronze.', 
                arquivo_origem STRING COMMENT 'Nome do arquivo JSON que originou os dados.'
            ) USING DELTA
        """)
        
        (df_lote.write
           .format("delta")
           .mode("append")
           .option("mergeSchema", "true") 
           .saveAsTable(tabela_full_name)
        )
        print(f"   ‚úÖ {qtd} previs√µes salvas.")
    else:
        print("   ‚ö†Ô∏è Nenhuma previs√£o neste lote.")

print(f"\nüèÅ Processamento finalizado. Total: {total_registros_geral}")